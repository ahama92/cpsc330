{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231e15e2-46da-4864-b32e-561778901fe7",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "When using a regression model, the maximum possible return value from .score(X,y) is 1. It can also have negative values.\n",
    "\n",
    "The top node in a decision tree is called the root.\n",
    "\n",
    "#### The following are for the train model.\n",
    "- Increasing the depth of the tree will increase the accuracy of the model.\n",
    "- Increasing the minimum number of samples needed to split on a feature will decrease the accuracy.\n",
    "- It's hard to predict the effect of both `max_depth` and `min_samples_split` at the same time.\n",
    "\n",
    "`.score()` is not calculated in the same way for regressions and classification problems.\n",
    "\n",
    "The same hyperparameters  from `DecisionTreeClassifier` can be used for `DecisionTreeRegressor()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c2971-3a70-4387-918a-5b4ff1aea0e9",
   "metadata": {},
   "source": [
    "`StandardScaler` ensures a fixed range (i.e., minimum and maximum values) for the features. False\n",
    "\n",
    "`StandardScaler` calculates mean and standard deviation for each feature separately. True\n",
    "\n",
    "In general, it’s a good idea to apply scaling on numeric features before training -NN or SVM RBF models. True\n",
    "\n",
    "The transformers such as `StandardScaler` or `SimpleImputer` in scikit-learn return a dataframe with transformed features. False, they return numpy array.\n",
    "\n",
    "The transformed feature values might be hard to interpret for humans. True (doesn't make sense but that's what she said)\n",
    "\n",
    "After applying `SimpleImputer` The transformed data has a different shape than the original data. False, only the values are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040698b-d87d-4aaa-9d09-d83fd8afe86e",
   "metadata": {},
   "source": [
    "You can “glue” together imputation and scaling of numeric features and scikit-learn classifier object within a single pipeline. True\n",
    "\n",
    "You can “glue” together scaling of numeric features, one-hot encoding of categorical features, and scikit-learn classifier object within a single pipeline. False\n",
    "\n",
    "Once you have a scikit-learn pipeline object you can call fit, predict, and score on it. True\n",
    "\n",
    "You can carry out data splitting within scikit-learn pipeline. False\n",
    "\n",
    "We have to be careful of the order we put each transformation and model in a pipeline. True\n",
    "\n",
    "Pipelines will fit and transform on the training fold and only transform on the validation fold during cross-validation. True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0a4b1-4a6b-4778-84c1-3feaaf4aa0a0",
   "metadata": {},
   "source": [
    "### Test data should never, under any circumstances, be used to train a model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
