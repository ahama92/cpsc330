{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lecture 6: `sklearn` `ColumnTransformer` and Text Features\n",
    "\n",
    "UBC 2021-22\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "from plotting_functions import *\n",
    "from utils import *\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- use `ColumnTransformer` to build all our transformations together into one object and use it with `sklearn` pipelines.  \n",
    "- explain `handle_unknown=\"ignore\"` hyperparameter of `scikit-learn`'s `OneHotEncoder`;\n",
    "- identify when it's appropriate to apply ordinal encoding vs one-hot encoding;\n",
    "- explain strategies to deal with categorical variables with too many categories; \n",
    "- explain why text data needs a different treatment than categorical variables;\n",
    "- use `scikit-learn`'s `CountVectorizer` to encode text data;\n",
    "- explain different hyperparameters of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In most applications, some features are categorical, some are continuous, some are binary, and some are ordinal. \n",
    "\n",
    "- When we want to develop supervised machine learning pipelines on real-world datasets, very often we want to apply different transformation on different columns. \n",
    "\n",
    "- Enter `sklearn`'s `ColumnTransformer`!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Let's look at a toy example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy_course</th>\n",
       "      <th>ml_experience</th>\n",
       "      <th>major</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab2</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>quiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>Economics</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>73.0</td>\n",
       "      <td>68</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>88.0</td>\n",
       "      <td>89</td>\n",
       "      <td>88</td>\n",
       "      <td>91</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>93.0</td>\n",
       "      <td>69</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Average</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>82.0</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>85</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>86.0</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>78</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Average</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>85.0</td>\n",
       "      <td>67</td>\n",
       "      <td>94</td>\n",
       "      <td>92</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Average</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Economics</td>\n",
       "      <td>Average</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>Biology</td>\n",
       "      <td>Good</td>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>94.0</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>87</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enjoy_course  ml_experience                   major class_attendance  \\\n",
       "0           yes              1        Computer Science        Excellent   \n",
       "1           yes              1  Mechanical Engineering          Average   \n",
       "2           yes              0             Mathematics             Poor   \n",
       "3            no              0             Mathematics        Excellent   \n",
       "4           yes              0              Psychology             Good   \n",
       "5            no              1               Economics             Good   \n",
       "6           yes              1        Computer Science        Excellent   \n",
       "7            no              0  Mechanical Engineering             Poor   \n",
       "8            no              0             Linguistics          Average   \n",
       "9           yes              1             Mathematics          Average   \n",
       "10          yes              0              Psychology             Good   \n",
       "11          yes              1                 Physics          Average   \n",
       "12          yes              1                 Physics        Excellent   \n",
       "13          yes              0  Mechanical Engineering        Excellent   \n",
       "14           no              0             Mathematics             Poor   \n",
       "15           no              1        Computer Science             Good   \n",
       "16          yes              0        Computer Science          Average   \n",
       "17          yes              1               Economics          Average   \n",
       "18           no              1                 Biology             Good   \n",
       "19           no              0              Psychology             Poor   \n",
       "20          yes              1             Linguistics        Excellent   \n",
       "\n",
       "    university_years  lab1  lab2  lab3  lab4  quiz1   quiz2  \n",
       "0                  3    92  93.0    84    91     92      A+  \n",
       "1                  2    94  90.0    80    83     91  not A+  \n",
       "2                  3    78  85.0    83    80     80  not A+  \n",
       "3                  3    91   NaN    92    91     89      A+  \n",
       "4                  4    77  83.0    90    92     85      A+  \n",
       "5                  5    70  73.0    68    74     71  not A+  \n",
       "6                  4    80  88.0    89    88     91      A+  \n",
       "7                  3    95  93.0    69    79     75  not A+  \n",
       "8                  2    97  90.0    94    82     80  not A+  \n",
       "9                  4    95  82.0    94    94     85  not A+  \n",
       "10                 3    98  86.0    95    95     78      A+  \n",
       "11                 1    95  88.0    93    92     85      A+  \n",
       "12                 2    98  96.0    96    99    100      A+  \n",
       "13                 4    95  94.0    96    95    100      A+  \n",
       "14                 3    95  90.0    93    95     70  not A+  \n",
       "15                 3    92  85.0    67    94     92  not A+  \n",
       "16                 5    75  91.0    93    86     85      A+  \n",
       "17                 3    86  89.0    65    86     87  not A+  \n",
       "18                 2    91   NaN    90    88     82  not A+  \n",
       "19                 2    77  94.0    87    81     89  not A+  \n",
       "20                 4    96  92.0    92    96     87      A+  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/quiz2-grade-toy-col-transformer.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   enjoy_course      21 non-null     object \n",
      " 1   ml_experience     21 non-null     int64  \n",
      " 2   major             21 non-null     object \n",
      " 3   class_attendance  21 non-null     object \n",
      " 4   university_years  21 non-null     int64  \n",
      " 5   lab1              21 non-null     int64  \n",
      " 6   lab2              19 non-null     float64\n",
      " 7   lab3              21 non-null     int64  \n",
      " 8   lab4              21 non-null     int64  \n",
      " 9   quiz1             21 non-null     int64  \n",
      " 10  quiz2             21 non-null     object \n",
      "dtypes: float64(1), int64(6), object(4)\n",
      "memory usage: 1.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformations on the toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy_course</th>\n",
       "      <th>ml_experience</th>\n",
       "      <th>major</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab2</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>quiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  enjoy_course  ml_experience                   major class_attendance  \\\n",
       "0          yes              1        Computer Science        Excellent   \n",
       "1          yes              1  Mechanical Engineering          Average   \n",
       "2          yes              0             Mathematics             Poor   \n",
       "3           no              0             Mathematics        Excellent   \n",
       "4          yes              0              Psychology             Good   \n",
       "\n",
       "   university_years  lab1  lab2  lab3  lab4  quiz1   quiz2  \n",
       "0                 3    92  93.0    84    91     92      A+  \n",
       "1                 2    94  90.0    80    83     91  not A+  \n",
       "2                 3    78  85.0    83    80     80  not A+  \n",
       "3                 3    91   NaN    92    91     89      A+  \n",
       "4                 4    77  83.0    90    92     85      A+  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Scaling on numeric features\n",
    "- One-hot encoding on the categorical feature `major` and binary feature `enjoy_class`\n",
    "- Ordinal encoding on the ordinal feature `class_attendance`\n",
    "- Imputation on the `lab2` feature\n",
    "- None on the `ml_experience` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `ColumnTransformer` example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['enjoy_course', 'ml_experience', 'major', 'class_attendance',\n",
       "       'university_years', 'lab1', 'lab2', 'lab3', 'lab4', 'quiz1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=[\"quiz2\"])\n",
    "y = df[\"quiz2\"]\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Identify the transformations we want to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enjoy_course</th>\n",
       "      <th>ml_experience</th>\n",
       "      <th>major</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab2</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  enjoy_course  ml_experience                   major class_attendance  \\\n",
       "0          yes              1        Computer Science        Excellent   \n",
       "1          yes              1  Mechanical Engineering          Average   \n",
       "2          yes              0             Mathematics             Poor   \n",
       "3           no              0             Mathematics        Excellent   \n",
       "4          yes              0              Psychology             Good   \n",
       "\n",
       "   university_years  lab1  lab2  lab3  lab4  quiz1  \n",
       "0                 3    92  93.0    84    91     92  \n",
       "1                 2    94  90.0    80    83     91  \n",
       "2                 3    78  85.0    83    80     80  \n",
       "3                 3    91   NaN    92    91     89  \n",
       "4                 4    77  83.0    90    92     85  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\"university_years\", \"lab1\", \"lab3\", \"lab4\", \"quiz1\"]  # apply scaling\n",
    "categorical_feats = [\"major\"]  # apply one-hot encoding\n",
    "passthrough_feats = [\"ml_experience\"]  # do not apply any transformation\n",
    "drop_feats = [\"lab2\", \"class_attendance\",'enjoy_course'] # do not include these features in modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's only focus on scaling and one-hot encoding first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Create a column transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each transformation is specified by a name, a transformer object, and the columns this transformer should be applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"scaling\", StandardScaler(), numeric_feats),\n",
    "        (\"onehot\", OneHotEncoder(sparse=False), categorical_feats),                        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Convenient `make_column_transformer` syntax\n",
    "\n",
    "- Similar to `make_pipeline` syntax, there is convenient `make_column_transformer` syntax. \n",
    "- The syntax automatically names each step based on its class. \n",
    "- We'll be mostly using this syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats),  # scaling on numeric features\n",
    "    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n",
    "    (\"drop\", drop_feats),  # drop the drop features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n",
       "                                 ['university_years', 'lab1', 'lab3', 'lab4',\n",
       "                                  'quiz1']),\n",
       "                                ('onehotencoder', OneHotEncoder(), ['major']),\n",
       "                                ('passthrough', 'passthrough',\n",
       "                                 ['ml_experience']),\n",
       "                                ('drop', 'drop',\n",
       "                                 ['lab2', 'class_attendance', 'enjoy_course'])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "transformed = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "- When we `fit_transform`, each transformer is applied to the specified columns and the result of the transformations are concatenated horizontally. \n",
    "- A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data.\n",
    "- Otherwise we might, for example, do the OHE on both train and test but forget to scale the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's examine the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(transformed[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09345386,  0.3589134 , -0.21733442,  0.36269995,  0.84002795,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-1.07471942,  0.59082668, -0.61420598, -0.85597188,  0.71219761,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.09345386, -1.26447953, -0.31655231, -1.31297381, -0.69393613,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.09345386,  0.24295676,  0.57640869,  0.36269995,  0.45653693,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.8878117 , -1.38043616,  0.37797291,  0.51503393, -0.05478443,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 1.86907725, -2.19213263, -1.80482065, -2.22697768, -1.84440919,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [ 0.8878117 , -1.03256625,  0.27875502, -0.09430199,  0.71219761,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.09345386,  0.70678332, -1.70560276, -1.46530779, -1.33308783,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-1.07471942,  0.93869659,  0.77484447, -1.00830586, -0.69393613,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.8878117 ,  0.70678332,  0.77484447,  0.81970188, -0.05478443,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-0.09345386,  1.05465323,  0.87406235,  0.97203586, -0.94959681,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [-2.05598498,  0.70678332,  0.67562658,  0.51503393, -0.05478443,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [-1.07471942,  1.05465323,  0.97328024,  1.58137177,  1.86267067,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  1.        ],\n",
       "       [ 0.8878117 ,  0.70678332,  0.97328024,  0.97203586,  1.86267067,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.09345386,  0.70678332,  0.67562658,  0.97203586, -1.97223953,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.09345386,  0.3589134 , -1.90403853,  0.81970188,  0.84002795,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [ 1.86907725, -1.61234944,  0.67562658, -0.39896994, -0.05478443,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.09345386, -0.33682642, -2.10247431, -0.39896994,  0.20087625,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-1.07471942,  0.24295676,  0.37797291, -0.09430199, -0.43827545,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ],\n",
       "       [-1.07471942, -1.38043616,  0.08031924, -1.16063983,  0.45653693,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.8878117 ,  0.82273995,  0.57640869,  1.12436984,  0.20087625,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that the returned object is not a dataframe. So there are no column names. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Viewing the transformed data as a dataframe\n",
    "\n",
    "- How can we view our transformed data as a dataframe? \n",
    "- We are adding more columns. \n",
    "- So the original columns won't directly map to the transformed data. \n",
    "- Let's create column names for the transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['university_years',\n",
       " 'lab1',\n",
       " 'lab3',\n",
       " 'lab4',\n",
       " 'quiz1',\n",
       " 'x0_Biology',\n",
       " 'x0_Computer Science',\n",
       " 'x0_Economics',\n",
       " 'x0_Linguistics',\n",
       " 'x0_Mathematics',\n",
       " 'x0_Mechanical Engineering',\n",
       " 'x0_Physics',\n",
       " 'x0_Psychology',\n",
       " 'ml_experience']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = (\n",
    "    numeric_feats\n",
    "    + ct.named_transformers_[\"onehotencoder\"].get_feature_names().tolist()\n",
    "    + passthrough_feats\n",
    ")\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'standardscaler': StandardScaler(),\n",
       " 'onehotencoder': OneHotEncoder(),\n",
       " 'passthrough': 'passthrough',\n",
       " 'drop': 'drop'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct.named_transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that the order of the columns in the transformed data depends upon the order of the features we pass to the `ColumnTransformer` and can be different than the order of the features in the original dataframe.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>x0_Biology</th>\n",
       "      <th>x0_Computer Science</th>\n",
       "      <th>x0_Economics</th>\n",
       "      <th>x0_Linguistics</th>\n",
       "      <th>x0_Mathematics</th>\n",
       "      <th>x0_Mechanical Engineering</th>\n",
       "      <th>x0_Physics</th>\n",
       "      <th>x0_Psychology</th>\n",
       "      <th>ml_experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>0.358913</td>\n",
       "      <td>-0.217334</td>\n",
       "      <td>0.362700</td>\n",
       "      <td>0.840028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.074719</td>\n",
       "      <td>0.590827</td>\n",
       "      <td>-0.614206</td>\n",
       "      <td>-0.855972</td>\n",
       "      <td>0.712198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>-1.264480</td>\n",
       "      <td>-0.316552</td>\n",
       "      <td>-1.312974</td>\n",
       "      <td>-0.693936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>0.242957</td>\n",
       "      <td>0.576409</td>\n",
       "      <td>0.362700</td>\n",
       "      <td>0.456537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.887812</td>\n",
       "      <td>-1.380436</td>\n",
       "      <td>0.377973</td>\n",
       "      <td>0.515034</td>\n",
       "      <td>-0.054784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.869077</td>\n",
       "      <td>-2.192133</td>\n",
       "      <td>-1.804821</td>\n",
       "      <td>-2.226978</td>\n",
       "      <td>-1.844409</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.887812</td>\n",
       "      <td>-1.032566</td>\n",
       "      <td>0.278755</td>\n",
       "      <td>-0.094302</td>\n",
       "      <td>0.712198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>0.706783</td>\n",
       "      <td>-1.705603</td>\n",
       "      <td>-1.465308</td>\n",
       "      <td>-1.333088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.074719</td>\n",
       "      <td>0.938697</td>\n",
       "      <td>0.774844</td>\n",
       "      <td>-1.008306</td>\n",
       "      <td>-0.693936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.887812</td>\n",
       "      <td>0.706783</td>\n",
       "      <td>0.774844</td>\n",
       "      <td>0.819702</td>\n",
       "      <td>-0.054784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>1.054653</td>\n",
       "      <td>0.874062</td>\n",
       "      <td>0.972036</td>\n",
       "      <td>-0.949597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-2.055985</td>\n",
       "      <td>0.706783</td>\n",
       "      <td>0.675627</td>\n",
       "      <td>0.515034</td>\n",
       "      <td>-0.054784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.074719</td>\n",
       "      <td>1.054653</td>\n",
       "      <td>0.973280</td>\n",
       "      <td>1.581372</td>\n",
       "      <td>1.862671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.887812</td>\n",
       "      <td>0.706783</td>\n",
       "      <td>0.973280</td>\n",
       "      <td>0.972036</td>\n",
       "      <td>1.862671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>0.706783</td>\n",
       "      <td>0.675627</td>\n",
       "      <td>0.972036</td>\n",
       "      <td>-1.972240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>0.358913</td>\n",
       "      <td>-1.904039</td>\n",
       "      <td>0.819702</td>\n",
       "      <td>0.840028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.869077</td>\n",
       "      <td>-1.612349</td>\n",
       "      <td>0.675627</td>\n",
       "      <td>-0.398970</td>\n",
       "      <td>-0.054784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.093454</td>\n",
       "      <td>-0.336826</td>\n",
       "      <td>-2.102474</td>\n",
       "      <td>-0.398970</td>\n",
       "      <td>0.200876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.074719</td>\n",
       "      <td>0.242957</td>\n",
       "      <td>0.377973</td>\n",
       "      <td>-0.094302</td>\n",
       "      <td>-0.438275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.074719</td>\n",
       "      <td>-1.380436</td>\n",
       "      <td>0.080319</td>\n",
       "      <td>-1.160640</td>\n",
       "      <td>0.456537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.887812</td>\n",
       "      <td>0.822740</td>\n",
       "      <td>0.576409</td>\n",
       "      <td>1.124370</td>\n",
       "      <td>0.200876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    university_years      lab1      lab3      lab4     quiz1  x0_Biology  \\\n",
       "0          -0.093454  0.358913 -0.217334  0.362700  0.840028         0.0   \n",
       "1          -1.074719  0.590827 -0.614206 -0.855972  0.712198         0.0   \n",
       "2          -0.093454 -1.264480 -0.316552 -1.312974 -0.693936         0.0   \n",
       "3          -0.093454  0.242957  0.576409  0.362700  0.456537         0.0   \n",
       "4           0.887812 -1.380436  0.377973  0.515034 -0.054784         0.0   \n",
       "5           1.869077 -2.192133 -1.804821 -2.226978 -1.844409         0.0   \n",
       "6           0.887812 -1.032566  0.278755 -0.094302  0.712198         0.0   \n",
       "7          -0.093454  0.706783 -1.705603 -1.465308 -1.333088         0.0   \n",
       "8          -1.074719  0.938697  0.774844 -1.008306 -0.693936         0.0   \n",
       "9           0.887812  0.706783  0.774844  0.819702 -0.054784         0.0   \n",
       "10         -0.093454  1.054653  0.874062  0.972036 -0.949597         0.0   \n",
       "11         -2.055985  0.706783  0.675627  0.515034 -0.054784         0.0   \n",
       "12         -1.074719  1.054653  0.973280  1.581372  1.862671         0.0   \n",
       "13          0.887812  0.706783  0.973280  0.972036  1.862671         0.0   \n",
       "14         -0.093454  0.706783  0.675627  0.972036 -1.972240         0.0   \n",
       "15         -0.093454  0.358913 -1.904039  0.819702  0.840028         0.0   \n",
       "16          1.869077 -1.612349  0.675627 -0.398970 -0.054784         0.0   \n",
       "17         -0.093454 -0.336826 -2.102474 -0.398970  0.200876         0.0   \n",
       "18         -1.074719  0.242957  0.377973 -0.094302 -0.438275         1.0   \n",
       "19         -1.074719 -1.380436  0.080319 -1.160640  0.456537         0.0   \n",
       "20          0.887812  0.822740  0.576409  1.124370  0.200876         0.0   \n",
       "\n",
       "    x0_Computer Science  x0_Economics  x0_Linguistics  x0_Mathematics  \\\n",
       "0                   1.0           0.0             0.0             0.0   \n",
       "1                   0.0           0.0             0.0             0.0   \n",
       "2                   0.0           0.0             0.0             1.0   \n",
       "3                   0.0           0.0             0.0             1.0   \n",
       "4                   0.0           0.0             0.0             0.0   \n",
       "5                   0.0           1.0             0.0             0.0   \n",
       "6                   1.0           0.0             0.0             0.0   \n",
       "7                   0.0           0.0             0.0             0.0   \n",
       "8                   0.0           0.0             1.0             0.0   \n",
       "9                   0.0           0.0             0.0             1.0   \n",
       "10                  0.0           0.0             0.0             0.0   \n",
       "11                  0.0           0.0             0.0             0.0   \n",
       "12                  0.0           0.0             0.0             0.0   \n",
       "13                  0.0           0.0             0.0             0.0   \n",
       "14                  0.0           0.0             0.0             1.0   \n",
       "15                  1.0           0.0             0.0             0.0   \n",
       "16                  1.0           0.0             0.0             0.0   \n",
       "17                  0.0           1.0             0.0             0.0   \n",
       "18                  0.0           0.0             0.0             0.0   \n",
       "19                  0.0           0.0             0.0             0.0   \n",
       "20                  0.0           0.0             1.0             0.0   \n",
       "\n",
       "    x0_Mechanical Engineering  x0_Physics  x0_Psychology  ml_experience  \n",
       "0                         0.0         0.0            0.0            1.0  \n",
       "1                         1.0         0.0            0.0            1.0  \n",
       "2                         0.0         0.0            0.0            0.0  \n",
       "3                         0.0         0.0            0.0            0.0  \n",
       "4                         0.0         0.0            1.0            0.0  \n",
       "5                         0.0         0.0            0.0            1.0  \n",
       "6                         0.0         0.0            0.0            1.0  \n",
       "7                         1.0         0.0            0.0            0.0  \n",
       "8                         0.0         0.0            0.0            0.0  \n",
       "9                         0.0         0.0            0.0            1.0  \n",
       "10                        0.0         0.0            1.0            0.0  \n",
       "11                        0.0         1.0            0.0            1.0  \n",
       "12                        0.0         1.0            0.0            1.0  \n",
       "13                        1.0         0.0            0.0            0.0  \n",
       "14                        0.0         0.0            0.0            0.0  \n",
       "15                        0.0         0.0            0.0            1.0  \n",
       "16                        0.0         0.0            0.0            0.0  \n",
       "17                        0.0         0.0            0.0            1.0  \n",
       "18                        0.0         0.0            0.0            1.0  \n",
       "19                        0.0         0.0            1.0            0.0  \n",
       "20                        0.0         0.0            0.0            1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### `ColumnTransformer`: Transformed data\n",
    "\n",
    "<br>\n",
    "<img src='./img/column-transformer.png' width=\"1500\">\n",
    "\n",
    "[Adapted from here.](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training models with transformed data\n",
    "- We can now pass the `ColumnTransformer` object as a step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A+', 'not A+', 'not A+', 'A+', 'A+', 'not A+', 'A+', 'not A+',\n",
       "       'not A+', 'A+', 'A+', 'A+', 'A+', 'A+', 'not A+', 'not A+', 'A+',\n",
       "       'not A+', 'not A+', 'not A+', 'A+'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, SVC())\n",
    "pipe.fit(X, y)\n",
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More on feature transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including `lab2`\n",
    "\n",
    "- Recall that `lab2` has missing values. \n",
    "- So we would like to apply more than one transformations, imputation and scaling, on it. \n",
    "- We can treat `lab2` separately, but we can also include it into `numeric_feats` and apply both transformations on all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\n",
    "    \"university_years\",\n",
    "    \"lab1\",\n",
    "    \"lab2\",\n",
    "    \"lab3\",\n",
    "    \"lab4\",\n",
    "    \"quiz1\",\n",
    "]  # apply scaling\n",
    "categorical_feats = [\"major\"]  # apply one-hot encoding\n",
    "passthrough_feats = [\"ml_experience\"]  # do not apply any transformation\n",
    "drop_feats = [\"class_attendance\",\"enjoy_course\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To apply more than one transformations we can define a pipeline inside a column transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(SimpleImputer(), StandardScaler()),\n",
    "        numeric_feats,\n",
    "    ),  # scaling on numeric features\n",
    "    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n",
    "    (\"drop\", drop_feats),  # drop the drop features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "column_names = (\n",
    "    numeric_feats\n",
    "    + ct.named_transformers_[\"onehotencoder\"].get_feature_names().tolist()\n",
    "    + passthrough_feats\n",
    ")\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating ordinal feature `class_attendance` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `class_attendance` column is different than the `major` column in that there is some ordering of the values. \n",
    "    - Excellent > Good > poor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy = X[[\"class_attendance\"]]\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(X_toy)\n",
    "X_toy_ord = enc.transform(X_toy)\n",
    "df = pd.DataFrame(\n",
    "    data=X_toy_ord,\n",
    "    columns=[\"class_attendance_enc\"],\n",
    "    index=X_toy.index,\n",
    ")\n",
    "pd.concat([X_toy, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The encoder doesn't know the order and we have to incorporate human knowledge about the ordering here. \n",
    "    - Excellent: 3\n",
    "    - Good: 2\n",
    "    - Average: 1\n",
    "    - Poor: 0\n",
    "- If you reverse the order it wouldn't matter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_toy[\"class_attendance\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's order them manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_attendance_levels = [\"Poor\", \"Average\", \"Good\", \"Excellent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(class_attendance_levels) == set(X_toy[\"class_attendance\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder(categories=[class_attendance_levels], dtype=int)\n",
    "oe.fit(X_toy[[\"class_attendance\"]])\n",
    "ca_transformed = oe.transform(X_toy[[\"class_attendance\"]])\n",
    "df = pd.DataFrame(\n",
    "    data=ca_transformed, columns=[\"class_attendance_enc\"], index=X_toy.index\n",
    ")\n",
    "print(oe.categories_)\n",
    "pd.concat([X_toy, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\n",
    "    \"university_years\",\n",
    "    \"lab1\",\n",
    "    \"lab2\",\n",
    "    \"lab3\",\n",
    "    \"lab4\",\n",
    "    \"quiz1\",\n",
    "]  # apply scaling\n",
    "categorical_feats = [\"major\"]  # apply one-hot encoding\n",
    "ordinal_feats = [\"class_attendance\"]  # apply ordinal encoding\n",
    "passthrough_feats = [\"ml_experience\"]  # do not apply any transformation\n",
    "drop_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(SimpleImputer(), StandardScaler()),\n",
    "        numeric_feats,\n",
    "    ),  # scaling on numeric features\n",
    "    (OneHotEncoder(), categorical_feats),  # OHE on categorical features\n",
    "    (\n",
    "        OrdinalEncoder(categories=[class_attendance_levels], dtype=int),\n",
    "        ordinal_feats,\n",
    "    ),  # Ordinal encoding on ordinal features\n",
    "    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n",
    "    (\"drop\", drop_feats),  # drop the drop features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "column_names = (\n",
    "    numeric_feats\n",
    "    + ct.named_transformers_[\"onehotencoder\"].get_feature_names().tolist()\n",
    "    + ordinal_feats\n",
    "    + passthrough_feats\n",
    ")\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dealing with unknown categories `handle_unknown=\"ignore\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X, y, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What's going on here??\n",
    "- Let's look at the error message:\n",
    "`ValueError: Found unknown categories ['Psychology'] in column 0 during transform\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X[\"major\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There is only one instance of Psychology.\n",
    "- During cross-validation, this is getting put into the validation split.\n",
    "- By default, `OneHotEncoder` throws an error because you might want to know about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Simplest fix:\n",
    "- Pass `handle_unknown=\"ignore\"` argument to `OneHotEncoder`\n",
    "- It creates a row with all zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(SimpleImputer(), StandardScaler()),\n",
    "        numeric_feats,\n",
    "    ),  # scaling on numeric features\n",
    "    (\n",
    "        OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        categorical_feats,\n",
    "    ),  # OHE on categorical features\n",
    "    (\n",
    "        OrdinalEncoder(categories=[class_attendance_levels], dtype=int),\n",
    "        ordinal_feats,\n",
    "    ),  # Ordinal encoding on ordinal features\n",
    "    (\"passthrough\", passthrough_feats),  # no transformations on the binary features\n",
    "    (\"drop\", drop_feats),  # drop the drop features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X, y, cv=5, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With this approach, all unknown categories will be represented with all zeros and cross-validation is running OK now. \n",
    "\n",
    "Ask yourself the following questions when you work with categorical variables   \n",
    "- Do you want this behaviour? \n",
    "- Are you expecting to get many unknown categories? Do you want to be able to distinguish between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cases where it's OK to break the golden rule \n",
    "\n",
    "- If it's some fix number of categories. For example, if it's something like provinces in Canada or majors taught at UBC. We know the categories in advance and this is one of the cases where it might be OK to violate the golden rule and get a list of all possible values for the categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features with only two possible categories\n",
    "\n",
    "- Sometimes you have variables with only two possible categories. \n",
    "- If we apply `OheHotEncoder` on such columns, it'll create two columns, which seems wasteful, as we could represent all information in the column in just one column with say 0's and 1's with presence of absence of one of the category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = pd.DataFrame(\n",
    "    {\n",
    "        \"bin_col\": [\n",
    "            \"pass\",\n",
    "            \"fail\",\n",
    "            \"fail\",\n",
    "            \"pass\",\n",
    "            \"pass\",\n",
    "            \"pass\",\n",
    "            \"fail\",\n",
    "            \"pass\",\n",
    "            \"pass\",\n",
    "            \"pass\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_enc = OneHotEncoder(drop=\"if_binary\", dtype=int, sparse=False)\n",
    "ohe_enc.fit(toy_df)\n",
    "transformed = ohe_enc.transform(toy_df)\n",
    "pd.DataFrame(transformed, columns = ['bin_enc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ColumnTransformer` on the California housing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(\"data/housing.csv\")\n",
    "train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Some column values are mean/median but some are not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's add some new features to the dataset which could help predicting the target: `median_house_value`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    rooms_per_household=train_df[\"total_rooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    rooms_per_household=test_df[\"total_rooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    bedrooms_per_household=train_df[\"total_bedrooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    bedrooms_per_household=test_df[\"total_bedrooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    population_per_household=train_df[\"population\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    population_per_household=test_df[\"population\"] / test_df[\"households\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep both numeric and categorical columns in the data.\n",
    "X_train = train_df.drop(columns=[\"median_house_value\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the categorical and numeric columns\n",
    "numeric_features = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"rooms_per_household\",\n",
    "    \"bedrooms_per_household\",\n",
    "    \"population_per_household\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "target = \"median_income\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's build a pipeline for our dataset\n",
    "- create the preprocessing pipelines for both numeric and categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we `fit` with the preprocessor, it calls `fit` on _all_ the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp = preprocessor.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we transform with the preprocessor, it calls `transform` on _all_ the transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get the new names of the columns that were generated by the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names(\n",
    "    categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the numeric feature names gives us all the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features + list(\n",
    "    preprocessor.named_transformers_[\"onehotencoder\"].get_feature_names(\n",
    "        categorical_features\n",
    "    )\n",
    ")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = make_pipeline(preprocessor, SVR(gamma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['imp + scaling + ohe + SVR'] = mean_std_cross_val_scores(pipe, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that categorical features are different than free text features. Sometimes there are columns containing free text information and we we'll look at ways to deal with them later in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the targets?\n",
    "\n",
    "- Generally no need for this when doing classification. \n",
    "- In regression it makes sense in some cases. More on this later. \n",
    "- `sklearn` is fine with categorical labels ($y$-values) for classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OHE with many categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do we have enough data for rare categories to learn anything meaningful? \n",
    "- How about grouping them into bigger categories\n",
    "    - Example: \"South America\" or \"Asia\"\n",
    "- Or having \"other\" category for rare cases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do we actually want to use certain features for prediction?\n",
    "\n",
    "- Do you want to use `race` in prediction?\n",
    "- Remember that the systems you build are going to be used in some applications. \n",
    "- It's extremely important to be mindful of the consequences of including certain features in your predictive model. \n",
    "- I would just drop the feature to avoid racial biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical features (True or False)\n",
    "\n",
    "- `handle_unknown=\"ignore\"` would treat all unknown categories equally. \n",
    "- Creating groups of rarely occurring categories might overfit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding text data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ML algorithms we have seen so far prefer numeric and fixed length input that looks like this: \n",
    "\n",
    "$$X = \\begin{bmatrix}1.0 & 4.0 & \\ldots & & 3.0\\\\ 0.0 & 2.0 & \\ldots & & 6.0\\\\ 1.0 & 0.0 & \\ldots & & 0.0\\\\ \\end{bmatrix}$$ \n",
    "\n",
    "and \n",
    "$$y = \\begin{bmatrix}spam \\\\ non spam \\\\ spam \\end{bmatrix}$$\n",
    "\n",
    "- But what if we are only given data in the form of raw text and associated labels?\n",
    "- How can we represent such data into fixed number of features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spam/non-spam toy example\n",
    "\n",
    "Would you be able to apply the algorithms we have seen so far on the data that looks like this?\n",
    "\n",
    "$X = \\begin{bmatrix}\\text{\"URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!\",}\\\\ \\text{\"Lol your always so convincing.\"}\\\\ \\text{\"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now!\"}\\\\ \\end{bmatrix}$ \n",
    "\n",
    "and \n",
    "\n",
    "$y = \\begin{bmatrix}spam \\\\ non spam \\\\ spam \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In categorical features or ordinal features, we have fixed number of categories.\n",
    "- In text features such as above, each feature value (i.e., each text message) is going to be different. \n",
    "- How do we encode these feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of words (BOW) representation\n",
    "\n",
    "- One way is to use a simple bag of words (BOW) representation which involves two components. \n",
    "    - The vocabulary (all unique words in all documents) \n",
    "    - A value indicating either the presence or absence or the count of each word in the document. \n",
    "        \n",
    "<center>\n",
    "<img src='./img/bag-of-words.png' width=\"800\">\n",
    "</center>\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/4.pdf)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extracting BOW features using `scikit-learn`\n",
    "- `CountVectorizer`\n",
    "    - Converts a collection of text documents to a matrix of word counts.  \n",
    "    - Each row represents a \"document\" (e.g., a text message in our example). \n",
    "    - Each column represents a word in the vocabulary in the training data. \n",
    "    - Each cell represents how often the word occurs in the document. \n",
    "    \n",
    "    \n",
    "Note: In the NLP community a text data set is referred to as a **corpus** (plural: corpora).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = [\n",
    "    \"URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!\",\n",
    "    \"Lol you are always so convincing.\",\n",
    "    \"Nah I don't think he goes to usf, he lives around here though\",\n",
    "    \"URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!\",\n",
    "    \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\",\n",
    "    \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
    "]\n",
    "y = [\"spam\", \"non spam\", \"non spam\", \"spam\", \"spam\", \"non spam\"]\n",
    "vec = CountVectorizer()\n",
    "X_counts = vec.fit_transform(X)\n",
    "bow_df = pd.DataFrame(X_counts.toarray(), columns=sorted(vec.vocabulary_), index=X)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of elements: \", np.prod(X_counts.shape))\n",
    "print(\"The number of non-zero elements: \", X_counts.nnz)\n",
    "print(\n",
    "    \"Proportion of non-zero elements: %0.4f\" % (X_counts.nnz / np.prod(X_counts.shape))\n",
    ")\n",
    "print(\n",
    "    \"The value at cell 3,%d is: %d\"\n",
    "    % (vec.vocabulary_[\"jackpot\"], X_counts[3, vec.vocabulary_[\"jackpot\"]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why sparse matrices? \n",
    "\n",
    "- Most words do not appear in a given document.\n",
    "- We get massive computational savings if we only store the nonzero elements.\n",
    "- There is a bit of overhead, because we also need to store the locations:\n",
    "    - e.g. \"location (3,31): 1\".\n",
    "    \n",
    "- However, if the fraction of nonzero is small, this is a huge win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Question for you\n",
    "- What would happen if you apply `StandardScaler` on sparse data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### `OneHotEncoder` and sparse features \n",
    "- By default, `OneHotEncoder` also creates sparse features. \n",
    "- You could set `sparse=False` to get a regular `numpy` array. \n",
    "- If there are a huge number of categories, it may be beneficial to keep them sparse.\n",
    "- For smaller number of categories, it doesn't matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important hyperparameters of `CountVectorizer` \n",
    "\n",
    "- `binary`\n",
    "    - whether to use absence/presence feature values or counts\n",
    "- `max_features`\n",
    "    - only consider top `max_features` ordered by frequency in the corpus\n",
    "- `max_df`\n",
    "    - ignore features which occur in more than `max_df` documents \n",
    "- `min_df` \n",
    "    - ignore features which occur in less than `min_df` documents \n",
    "- `ngram_range`\n",
    "    - consider word sequences in the given range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at all features, i.e., words (along with their frequencies).\n",
    "vec_all = CountVectorizer()\n",
    "X_counts = vec_all.fit_transform(X)\n",
    "pd.DataFrame(\n",
    "    data=X_counts.sum(axis=0).tolist()[0],\n",
    "    index=vec_all.get_feature_names(),\n",
    "    columns=[\"counts\"],\n",
    ").sort_values(\"counts\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can control the size of X (the number of features) using `max_features`\n",
    "vec8 = CountVectorizer(max_features=8)\n",
    "X_counts = vec8.fit_transform(X)\n",
    "pd.DataFrame(\n",
    "    data=X_counts.sum(axis=0).tolist()[0],\n",
    "    index=vec8.get_feature_names(),\n",
    "    columns=[\"counts\"],\n",
    ").sort_values(\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(X_counts.toarray(), columns=sorted(vec8.vocabulary_), index=X)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec8_binary = CountVectorizer(binary=True, max_features=8)\n",
    "X_counts = vec8_binary.fit_transform(X)\n",
    "pd.DataFrame(\n",
    "    data=X_counts.sum(axis=0).tolist()[0],\n",
    "    index=vec8.get_feature_names(),\n",
    "    columns=[\"counts\"],\n",
    ").sort_values(\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that `vec8` and `vec8_binary` have different vocabularies, which is kind of unexpected behaviour and doesn't match the documentation of `scikit-learn`. \n",
    "\n",
    "[Here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L1206-L1225) is the code for `binary=True` condition in `scikit-learn`. As we can see, the binarization is done before limiting the features to `max_features`, and so now we are actually looking at the document counts (in how many documents it occurs) rather than term count. This is not explained anywhere in the documentation. \n",
    "\n",
    "The ties in counts between different words makes it even more confusing. I don't think it'll have a big impact on the results but this is good to know! Remember that `scikit-learn` developers are also humans who are prone to make mistakes. So it's always a good habit to question whatever tools we use every now and then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(\n",
    "    X_counts.toarray(), columns=sorted(vec8_binary.vocabulary_), index=X\n",
    ")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Note that `CountVectorizer` is carrying out some preprocessing such as because of the default argument values \n",
    "    - Converting words to lowercase (`lowercase=True`)\n",
    "    - getting rid of punctuation and special characters (`token_pattern ='(?u)\\\\b\\\\w\\\\w+\\\\b'`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is this a realistic representation of text data? \n",
    "\n",
    "- Of course this is not a great representation of language\n",
    "    - We are throwing out everything we know about language and losing a lot of information. \n",
    "    - It assumes that there is no syntax and compositional meaning in language.  \n",
    "- But it works surprisingly well for many tasks. \n",
    "- We will learn more expressive representations in the coming weeks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CountVectorizer`: True or False\n",
    "\n",
    "- As you increase the value for `max_features` hyperparameter of `CountVectorizer` the training score is likely to go up. \n",
    "- If we encounter a word in the validation or the test split that's not available in the training data, we'll get an error. \n",
    "- `max_df` hyperparameter of `CountVectorizer` can be used to get rid of most frequently occurring words from the dictionary.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did we learn today?\n",
    "\n",
    "- Motivation to use `ColumnTransformer`\n",
    "- `ColumnTransformer` syntax\n",
    "- Defining transformers with multiple transformations\n",
    "- How to visualize transformed features in a dataframe \n",
    "- More on ordinal features \n",
    "- Different arguments `OneHotEncoder`\n",
    "    - `handle_unknow=\"ignore\"`\n",
    "    - `if_binary`\n",
    "    - `sparse=False`\n",
    "- Dealing with text features\n",
    "    - Bag of words representation: `CountVectorizer`"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
